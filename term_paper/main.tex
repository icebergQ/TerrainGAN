\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{empheq}
\usepackage{wrapfig}

\usepackage{algorithm}
\usepackage{algpseudocode}
\algdef{SE}[SUBALG]{Indent}{EndIndent}{}{\algorithmicend\ }%
\algtext*{Indent}
\algtext*{EndIndent}
\newcommand\myeq{\stackrel{\mathclap{\normalfont\mbox{def}}}{=}}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Terrain GAN optimizer study}

\author{\IEEEauthorblockN{Kai Qin}
\IEEEauthorblockA{\textit{Dept. of Electrical and Computer Engineering} \\
\textit{University of Texas at Austin}\\
Austin, TX \\
kai.qin@utexas.edu}
\and
\IEEEauthorblockN{Yi Han}
\IEEEauthorblockA{\textit{Dept. of Electrical and Computer Engineering} \\
\textit{University of Texas at Austin}\\
Austin, TX \\
yh5598@utexas.edu}
}

\maketitle

%\begin{abstract}
%This report is for the term project of EE381V Introduction to Optimization.
%\end{abstract}

% \begin{IEEEkeywords}
% component, formatting, style, styling, insert
% \end{IEEEkeywords}

\section{Introduction}
%Generative Adversarial Networks (GAN)\cite{goodfellow2014generative} have been wildy used in generative applications such as anime generation, human-face generation, and video generation. In this study, we focus on evaluating the performence of three gradient-based optimization algorithms in training GANs for procedural terrain generation\cite{beckham2017step}. "Procedural terrain generation for video games has been traditionally done with smartly designed but handcrafted algorithms that generate hightmaps" \cite{beckham2017step} $\{DESCRIBE\  dataset\}$ The first goal of this project is for us as a term project to extend what we have learned in class about gradient-based optimization algorithms and to dive deeper in this topic by experimenting with three state-of-the-art gradient based algorithm for training neural networks. The second goal is to explore the practical differences that each optimizer can make for TerrianGAN. The third goal is to understands the results of fine-tuning parameters on the different optimizers and their effects on TerrianGAN.
Generative Adversarial Networks (GANs\cite{goodfellow2014generative}) have been wildly used in applications such as anime character generation, human-face generation, and video generation. In this study, we focused on evaluating the performances of three different gradient based optimizers on our TerrainGAN, explored the practical differences from using each optimizers, and discuss the results of fine-tuning hyper-parameters and visualize their effect on our GAN.

\section{Terrain GAN}
Traditionally, video game terrains have been either manually generated or procedually gendreatd by algorithms designed to mimic real-life terrains such as mountains, lakes, and coasts. These methods are capable of generating high quality terrains but also come with drawbacks such as high expense of human labour and the lack of flexibility and complexity of the nature. With the ability to recover the training data distribution \cite{goodfellow2014generative}, GAN becomes the perfect algorithm for this task. A first step towards conquering this problem using GAN has been proposed in \cite{beckham2017step}, where DCGAN and LSGAN have been applied to generate high quality heightmaps (see image \ref{theirheightmap}). In our project, we focus on evaluating the effects of different optimizers on LSGAN since it is suggested in the paper \cite{beckham2017step} that LSGAN provide better training stability than DCGAN.

Following \cite{beckham2017step}, we found an open sourced NASA grayscale topography map of Earth. The value of the each tile is grayscale, ranging from 0 to 255. Higher value in the grayscale represents higher altitude such as mountain tops, while lower values represent valley or sea level. The 21600x10800 pixels high resolution map is subsequently divided into 128x128 pixel tiles, which provides about 14200 training data for our GAN. However, since about $70\%$ of Earth is water and therefore uninteresting in the purpose of generating terrain with variations, tiles that are purely sea level are removed, leaving about 4300 training tiles fit for training.
\begin{figure}
    \centering
    \includegraphics[scale=0.4]{thereheightmap.PNG}
    \caption{A redering of a generated heightmap in \cite{beckham2017step}}
    \label{theirheightmap}
\end{figure}
One novelty of GAN is learning the training data distribution via an adversaria process. The training process consists of two steps as shown in the psudocode in figure \ref{ganalg}, where we first train the discriminator$G$ for $k$ steps, and then train the generator $D$ for one step. The number of steps to apply to the discriminator, $k$, is a hyperparameter. As in the GAN paper, we used $k=1$, the least expensive option, in all of our experiments in the next section.

DCGAN and LSGAN are two variants of GAN, where DCGAN improves the orginal GAN with fine-tuned architecture details and showed us that GAN is capable of generating perceptually good samples (see figure \ref{dcgan2} and interpolations \cite{radford2015unsupervised}. And LSGAN adopted the least squares loss instead of the cross entropy as the objective function since the original loss function may lead to the vanishing gradients problem. 

%We experimented linear interpolation with generated images but unable to visually tell what geographical features the trained GAN has learned.
\begin{figure}
    \centering
    \includegraphics[scale=1]{dcgan2.PNG}
    \caption{DCGAN samples on faces \cite{goodfellow2016deep}}
    \label{dcgan2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.45]{ganalg.PNG}
    \caption{Minibatch stochastic gradient descent training of GAN \cite{goodfellow2014generative}}
    \label{ganalg}
\end{figure}


\section{Gradient-based optimization algorithms review}

Before we dive into the evaluation of three gradient-based optimizers we compared in this study, SGD with momentum, RMSProp, and ADAM, we want to provide a review of these three algorithms. We will start from the classic gradient descent algorithm. In one sentence, the general idea of the classic gradient descent algorithm is that at every iteration, the desired parameters are moving in the direction of the negtive gradient of the objective function based on the entire training dataset to decrease the loss function. The vanilla gradient descent may be accelerated considerably by using stochastic gradient descent (SGD) which follows the gradient of randomly selected minibatches. Even though SGD ingtroduced this very important idea of training with minibatches, learning with it can sometimes be very slow. For example, let's say that you're trying to optimize a cost function which has a contour in image \ref{contour}.
The red dot denotes the position of the minimum. And we can see this up and own oscillations in black lines slows down the gradient descent. Therefore, it's rear to see large scale neural network training with classic SGD. One improvement made to SGD is by adding momentum.The basic idea of SGD with momentum is to compute an exponentially decaying moving average of the past gradients and continues to move in that direction\cite{goodfellow2016deep}. The velocity update step in algortihm \ref{sgdm} shows how to compute the exponentially weighted averages of the last N iterations, where N is determined by the hyperparameter alpha. For exambple, when alpha equals $0.99$, which is commonly used in practice, we are approximately averaging over the last 100 iterations. This method can only approximate the average value over the last N days, but it takes very little memory. Image \ref{contour}  illustrates the effect of momentum. If you average out these gradients of the black arrows, you can find that the oscillations in the forward diagonal direction will tend to average out to something closer to zero. In the mean time, it will keep the direvative on the backward diagonal direction. So this allows the optimizer to take a more straight foward path or to damp out the oscillations in the path to the minimum.
\begin{figure}
    \centering
    \includegraphics[scale=0.5]{contour.PNG}
    \caption{Visualization of the effect of momentum\cite{goodfellow2016deep}}
    \label{contour}
  \end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.45]{sgdm.PNG}
    \caption{SGD with momentum \cite{goodfellow2016deep}}
    \label{sgdm}
\end{figure}
%The hyper prameters we have here are learning rate, beta, beta2, batch size. 
%The SGD gradient estimator introduces a source of noise which makes the optimizer oscliates when we arrive around a minimum, thus it is common to decay the learning rate.

Root mean squared prop (RMSProp) is another algorithm that can speed up the classic gradient descent. This algorithm (see figure in \ref{rmsprop}) is very similar to SGD with momentum, but instead of accumulating the gradient, we are taking the average of the sqaured gradient and divide the gradient by the sqaure root of the average squared gradient\cite{goodfellow2016deep}. By dividing the average magnitude of the derivative in each dimenstion, the net effect is that the forward diagonal derivative in image \ref{contour} is devided by a relatively larger number, and it therefore helps damp out the oscillations. Comparing to SGD with momentum, we can use a larger learning rate, and get faster learning without diverging in the forward diagonal direction, where in SGD with momentum, the learning rate is applying to all each dimension with same step size.
\begin{figure}
    \centering
    \includegraphics[scale=0.35]{rmsprop.PNG}
    \caption{RMSProp \cite{goodfellow2016deep}}
    \label{rmsprop}
\end{figure}
% ADAM was used in the original paper of DC GAN and LSGAN, and thus was our first choice optimizer.


By combining both ideas of momentum and RMSProp, we get Adam optimization algorithm which has been shown to work well across a wider range of deep learning architectures. In Adam, in each iteration, in the first highlighted equation in figure \ref{adam} we first update the first moment estimate (the mean of the derivatives) , which is exactly what we had when we're implementing SGD with momentum. And similarly, we do the rms prop to update the second moment estimate in the second equation. Then we do bias corrections. Finally, we compute the update by dividing the bias corrected first moment estimate with the sqaure root of the second moment estimate and reversing the sign. Common choices for $\rho_1$, $\rho_2$, $\sigma$ are $0.9$, $0.99$ and $10^{-8}$ respectively. In this study, we followed the common pratice for Adam hyperparameter tunning which keeps $\rho_1$,  $\rho_2$, and $\sigma$ as default and try a range of values of the learning rate to see which works the best.
\begin{figure}
    \centering
    \includegraphics[scale=0.45]{adam.PNG}
    \caption{ADAM \cite{goodfellow2016deep}}
    \label{adam}
\end{figure}



\section{Experiments and results}
In this project, the training data are 128px height maps from the original NASA image
As we can see in the graph on the right, the loss function of both G and D oscilate and don't converge. That's one of the big challenge of GAN training is that you don't get this clean monotonic improvment that your are used to with training supervised models.In GANs, the objective function for the generator and the discriminator usually measures how well they are doing relative to the opponent. For example, we measure how well the generator is fooling the discriminator.


Sometime it oscilates and don't converge. That's one of the big challenge of GAN training is that you don't get this clean monotonic improvment that your are used to with training supervised models.


``It is still an open probelm to have good metrics. If you had a really really good metric, you can propabally use it as a optimiztion critieria and optimize against it'' ``Assuming you don't do optimization directly on the Frechet Inception Distance, it can be a nice independent measure of the amount of variantion and cristness of the image generated ; when you do optimize against it, you will find results not exactly what you want''
Adam 0.0002 lr and 16 batch size, we think based on the 3d model generated by this software looks realistic enough to meet our experience. We manually sweeped the lr rate and batch size using ADAM and collected the loss fuction over trainnign iterations. Next step will colecte results using other optimizers and figure out a metric to quantify the results we see. The discriminitor can always 

\section{Discussion about metrics}
As we have seen in the loss function graphs, the oscillation makes it harder to measure the success of the training progress. Currently, it is still an open problem to find good metrics for GAN training. Simply put, If you have a validation metric, you can probably use it as the objective function and optimize against it. Inception score \cite{salimans2016improved} and Frechet Inception Distance score (FID\cite{heusel2017gans}) are two state of the art metrics people use to measure GAN performances. However, both metrics donâ€™t apply to our training data. Inception score requires categorically labeled data and FID requires a pretrained classification neural network. Inspired by the goal of GAN to learn the probability distribution from the training data and implicitly represent it using neural networks, we propose a new metric to measure the TerrainGAN performance: the KL-divergence between normalized intensity histogram of training images and generated images.

Before we present how we calculated this metric, we need first introduce the idea of image intensity histogram, which is a widely used technique in image processing. The definition states the histograms plots how many times each intensity value in an image occurs. Here we have a example of intensity histogram of a generated heightmap in figure  \ref{intensityhist}. We then produce the distribution over a large set of images by calculating the average occurrences of each intensity value and divide it by the total number of pixels over all images, and defining it as the normalized histogram. The normalizatoin process also remaps the occurences to the same range $[0,1]$ as the probability distribution. Figure \ref{traininten} shows the normalized intensity histogram of the training images, and figure \ref{adaminten} shows the normalized intensity histogram of 50 generated image using the neural net trained by Adam with lr .0002 and batch-size 16 for 150 epochs. 
\begin{figure}
    \centering
    \includegraphics[scale=0.45]{intensityhist.PNG}
    \caption{Image intensity histogram of a generated height map.}
    \label{intensityhist}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.55]{trainingintensity.PNG}
    \caption{Normalized image intensity histogram of the training dataset and an example training image.}
    \label{traininten}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.55]{adamintensity.PNG}
    \caption{Normalized image intensity histogram of generated images from LSGAN trained by ADAM with $lr=0.0002$ and $batchsize=16$ and a sample image. KL divergence of normalized intensity histogram equals $1.62$}
    \label{adaminten}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.55]{sgdintensity.PNG}
    \caption{Normalized image intensity histogram of generated images from LSGAN trained by SGD with $lr=0.0001$ and $batchsize=16$ and a sample image. KL divergence of normalized intensity histogram equals $+\infty$}
    \label{sgdinten}
\end{figure}


Given the normalized image intensity histograms of the training data and generated samples, we can calculate the KL divergence using equation (\ref{kldiv}), where $P(x)$ is the training data distribution and $Q(x)$ is the generated sample disctribution. Using the training dataset normalized histogram in image \ref{traininten} as a reference distribution, image \ref{adaminten} shows a normalized histogram with KL-divegence of $1.62$ and image \ref{sgdinten} shows a normalized histogram with KL-divergence of $+\infty$. We get a positive infinity since the training data and the generated data in figure \ref{sgdinten} have no overlap in normalized histogram. 
\begin{equation}
\label{kldiv}
  D_{KL}(P||Q)=\sum_{x\in \chi}{P(x)log(P(x)/Q(x))}
\end{equation}
This metric has been utilized in measuring the quality of different hyperparameters when use Adam to train LSGAN. Figure \ref{klbar} shows a bar plot of the KL divergence of different hyperprameter settings of Adam after 150 ephchs. Visually, we  can see that the higher the metric, the harder you can provide some geographic meaning to the generated images.
\begin{figure}
    \centering
    \includegraphics[scale=0.45]{klbar.PNG}
    \caption{Normalized histogram KL divergence of generated samples vs. different Adam learning rate and batch size}
    \label{klbar}
\end{figure}

%Compare performence of each optimizer:
%we need a metric to tell if the generated image is good or bad.
%Based on our goal, we want noticeable feasures like mountains or lakes, valley, coasts.

\section{Conclusion}
%We definitely learned a lot about different optimizers and a lot about GAN, DCGAN, LSGAN. shou huo hen da
\section{Future Works}




\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,main}

\end{document}

