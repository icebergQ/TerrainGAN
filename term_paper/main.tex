\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{empheq}
\usepackage{wrapfig}

\usepackage{algorithm}
\usepackage{algpseudocode}
\algdef{SE}[SUBALG]{Indent}{EndIndent}{}{\algorithmicend\ }%
\algtext*{Indent}
\algtext*{EndIndent}
\newcommand\myeq{\stackrel{\mathclap{\normalfont\mbox{def}}}{=}}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Terrain GAN optimizer study}

\author{\IEEEauthorblockN{Kai Qin}
\IEEEauthorblockA{\textit{Dept. of Electrical and Computer Engineering} \\
\textit{University of Texas at Austin}\\
Austin, TX \\
kai.qin@utexas.edu}
\and
\IEEEauthorblockN{Yi Han}
\IEEEauthorblockA{\textit{Dept. of Electrical and Computer Engineering} \\
\textit{University of Texas at Austin}\\
Austin, TX \\
yh5598@utexas.edu}
}

\maketitle

%\begin{abstract}
%This report is for the term project of EE381V Introduction to Optimization.
%\end{abstract}

% \begin{IEEEkeywords}
% component, formatting, style, styling, insert
% \end{IEEEkeywords}

\section{Introduction}
% Generative Adversarial Networks (GAN)\cite{goodfellow2014generative} have been wildy used in generative applications such as anime generation, human-face generation, and video generation. In this study, we focus on evaluating the performence of three gradient-based optimization algorithms in training GANs for procedural terrain generation\cite{beckham2017step}. "Procedural terrain generation for video games has been traditionally done with smartly designed but handcrafted algorithms that generate hightmaps" \cite{beckham2017step} $\{DESCRIBE\  dataset\}$ The first goal of this project is for us as a term project to extend what we have learned in class about gradient-based optimization algorithms and to dive deeper in this topic by experimenting with three state-of-the-art gradient based algorithm for training neural networks. The second goal is to explore the practical differences that each optimizer can make for TerrianGAN. The third goal is to understands the results of fine-tuning parameters on the different optimizers and their effects on TerrianGAN.
Generative Adversarial Networks (GANs\cite{goodfellow2014generative}) have been wildly used in applications such as anime character generation, human-face generation, and video generation. In this study, we focused on evaluating the performances of three different gradient based optimizers on our TerrainGAN, explored the practical differences from using each optimizers, and discuss the results of fine-tuning hyper-parameters and visualize their effect on our GAN.

\section{Terrain GAN}
Traditionally, video game terrains have been either manually generated or procedually gendreatd by algorithms designed to mimic real-life terrains such as mountains, lakes, and coasts. These methods are capable of generating high quality terrains but also come with drawbacks such as high expense of human labour and the lack of flexibility and complexity of the nature. With the ability to recover the training data distribution \cite{goodfellow2014generative}, GAN becomes the perfect algorithm for this task. A first step towards conquering this problem using GAN has been proposed in \cite{beckham2017step}, where DCGAN and LSGAN have been applied to generate high quality heightmaps (see image \ref{theirheightmap}). In our project, we focus on evaluating the effects of different optimizers on LSGAN since it is suggested in the paper \cite{beckham2017step} that LSGAN provide better training stability than DCGAN.

Following \cite{beckham2017step}, we found an open sourced NASA grayscale topography map of Earth. The value of the each tile is grayscale, ranging from 0 to 255. Higher value in the grayscale represents higher altitude such as mountain tops, while lower values represent valley or sea level. The 21600x10800 pixels high resolution map is subsequently divided into 128x128 pixel tiles, which provides about 14200 training data for our GAN. However, since about $70\%$ of Earth is water and therefore uninteresting in the purpose of generating terrain with variations, tiles that are purely sea level are removed, leaving about 4300 training tiles fit for training.
\begin{figure}
    \centering
    \includegraphics[scale=0.4]{thereheightmap.PNG}
    \caption{A redering of a generated heightmap in \cite{beckham2017step}}
    \label{theirheightmap}
\end{figure}
One novelty of GAN is learning the training data distribution via an adversaria process. The training process consists of two steps as shown in the psudocode in figure \ref{ganalg}, where we first train the discriminator$G$ for $k$ steps, and then train the generator $D$ for one step. The number of steps to apply to the discriminator, $k$, is a hyperparameter. As in the GAN paper, we used $k=1$, the least expensive option, in all of our experiments in the next section.

DCGAN and LSGAN are two variants of GAN, where DCGAN improves the orginal GAN with fine-tuned architecture details and showed us that GAN is capable of generating perceptually good samples (see figure \ref{dcgan2} and interpolations \cite{radford2015unsupervised}. And LSGAN adopted the least squares loss instead of the cross entropy as the objective function since the original loss function may lead to the vanishing gradients problem. 

%We experimented linear interpolation with generated images but unable to visually tell what geographical features the trained GAN has learned.
\begin{figure}
    \centering
    \includegraphics[scale=1]{dcgan2.PNG}
    \caption{DCGAN samples on faces \cite{goodfellow2016deep}}
    \label{dcgan2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.45]{ganalg.PNG}
    \caption{Minibatch stochastic gradient descent training of GAN \cite{goodfellow2014generative}}
    \label{ganalg}
\end{figure}


\section{Gradient-based optimization algorithms review}

Before we dive into the evaluation of three gradient-based optimizers we compared in this study, SGD with momentum, RMSProp, and ADAM, we want to provide a review of these three algorithms. We will start from the classic gradient descent algorithm. In one sentence, the general idea of the classic gradient descent algorithm is that at every iteration, the desired parameters are moving in the direction of the negtive gradient of the objective function based on the entire training dataset to decrease the loss function. The vanilla gradient descent may be accelerated considerably by using stochastic gradient descent (SGD) which follows the gradient of randomly selected minibatches. Even though SGD ingtroduced this very important idea of training with minibatches, learning with it can sometimes be very slow. For example, let's say that you're trying to optimize a cost function which has a contour in image \ref{contour}.
The red dot denotes the position of the minimum. And we can see this up and own oscillations in black lines slows down the gradient descent. Therefore, it's rear to see large scale neural network training with classic SGD. One improvement made to SGD is by adding momentum.The basic idea of SGD with momentum is to compute an exponentially decaying moving average of the past gradients and continues to move in that direction\cite{goodfellow2016deep}. The velocity update step in algortihm \ref{sgdm} shows how to compute the exponentially weighted averages of the last N iterations, where N is determined by the hyperparameter alpha. For exambple, when alpha equals $0.99$, which is commonly used in practice, we are approximately averaging over the last 100 iterations. This method can only approximate the average value over the last N days, but it takes very little memory. Image \ref{contour}  illustrates the effect of momentum. If you average out these gradients of the black arrows, you can find that the oscillations in the forward diagonal direction will tend to average out to something closer to zero. In the mean time, it will keep the direvative on the backward diagonal direction. So this allows the optimizer to take a more straight foward path or to damp out the oscillations in the path to the minimum.
\begin{figure}
    \centering
    \includegraphics[scale=0.5]{contour.PNG}
    \caption{Visualization of the effect of momentum\cite{goodfellow2016deep}}
    \label{contour}
  \end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.45]{sgdm.PNG}
    \caption{SGD with momentum \cite{goodfellow2016deep}}
    \label{sgdm}
\end{figure}
%The hyper prameters we have here are learning rate, beta, beta2, batch size. 
%The SGD gradient estimator introduces a source of noise which makes the optimizer oscliates when we arrive around a minimum, thus it is common to decay the learning rate.

Root mean squared prop (RMSProp) is another algorithm that can speed up the classic gradient descent. This algorithm (see figure in \ref{rmsprop}) is very similar to SGD with momentum, but instead of accumulating the gradient, we are taking the average of the sqaured gradient and divide the gradient by the sqaure root of the average squared gradient\cite{goodfellow2016deep}. By dividing the average magnitude of the derivative in each dimenstion, the net effect is that the forward diagonal derivative in image \ref{contour} is devided by a relatively larger number, and it therefore helps damp out the oscillations. Comparing to SGD with momentum, we can use a larger learning rate, and get faster learning without diverging in the forward diagonal direction, where in SGD with momentum, the learning rate is applying to all each dimension with same step size.
\begin{figure}
    \centering
    \includegraphics[scale=0.35]{rmsprop.PNG}
    \caption{RMSProp \cite{goodfellow2016deep}}
    \label{rmsprop}
\end{figure}
% ADAM was used in the original paper of DC GAN and LSGAN, and thus was our first choice optimizer.


By combining both ideas of momentum and RMSProp, we get Adam optimization algorithm which has been shown to work well across a wider range of deep learning architectures. In Adam, in each iteration, in the first highlighted equation in figure \ref{adam} we first update the first moment estimate (the mean of the derivatives) , which is exactly what we had when we're implementing SGD with momentum. And similarly, we do the rms prop to update the second moment estimate in the second equation. Then we do bias corrections. Finally, we compute the update by dividing the bias corrected first moment estimate with the sqaure root of the second moment estimate and reversing the sign. Common choices for $\rho_1$, $\rho_2$, $\sigma$ are $0.9$, $0.99$ and $10^{-8}$ respectively. In this study, we followed the common pratice for Adam hyperparameter tunning which keeps $\rho_1$,  $\rho_2$, and $\sigma$ as default and try a range of values of the learning rate to see which works the best.
\begin{figure}
    \centering
    \includegraphics[scale=0.45]{adam.PNG}
    \caption{ADAM \cite{goodfellow2016deep}}
    \label{adam}
\end{figure}



\section{Experiments and results}
%In this project, the training data are 128px height maps from the original NASA image
%As we can see in the graph on the right, the loss function of both G and D oscilate and don't converge. That's one of the big challenge of GAN training is that you don't get this clean monotonic improvment that your are used to with training supervised models.In GANs, the objective function for the generator and the discriminator usually measures how well they are doing relative to the opponent. For example, we measure how well the generator is fooling the discriminator.
%Sometime it oscilates and don't converge. That's one of the big challenge of GAN training is that you don't get this clean monotonic improvment that your are used to with training supervised models.
%``It is still an open probelm to have good metrics. If you had a really really good metric, you can propabally use it as a optimiztion critieria and optimize against it'' ``Assuming you don't do optimization directly on the Frechet Inception Distance, it can be a nice independent measure of the amount of variantion and cristness of the image generated ; when you do optimize against it, you will find results not exactly what you want''
%Adam 0.0002 lr and 16 batch size, we think based on the 3d model generated by this software looks realistic enough to meet our experience. We manually sweeped the lr rate and batch size using ADAM and collected the loss fuction over trainnign iterations. Next step will colecte results using other optimizers and figure out a metric to quantify the results we see. The discriminitor can always 
All experiments of TerrainGAN takes 150 epochs, which means 150 iterations through the entire ~4300 training images, to complete. Generator outputs throughout the training process are periodically saved to visualize convergence and success. Generator/discriminator parameters, loss function plot, and training time are also saved for further analysis. Training was completed on a 2080Ti GPU and 32GB of RAM for hardware reference.
\begin{figure}
    \centering
    \includegraphics[scale=0.4]{3catsamples.PNG}
    \caption{Sample generated output from different training stages}
    \label{3catsamples}
\end{figure}
We purposefully imported the grayscale images as colored RGB images into the training dataset to provide an extra way of visualize optimizer performance. Good optimizers will eliminate color at the very beginning of training. Similarly, we did not change input latent variable size from 100 to a number that is a power of 2 to visualize the checkerboard effect [?], which signals inferior convergence.

\subsection*{Fine tuning adam optimizer}
The most visually diverse and successful result was ran using the Adam optimizer with learning rate 0.0002 and batch size 16.
\begin{figure}
    \centering
    \includegraphics[scale=0.4]{adamresults.PNG}
    \caption{Sample output from Adam optimizer (left), loss function graph of generator and discriminator (right)}
    \label{adamresults}
\end{figure}
As we can see in the loss function graph on the right, the loss function of both generator G and discriminator D oscillate and don’t converge. One of the bigger challenges in GAN training is that you don’t get a clean monotonic improvement curve produced with training supervised models. In GANs, the objective function for the generator and the discriminator usually measures how well they are doing relative to the opponent. We confirm how well the generator is fooling the discriminator by observing a consistent oscillating behavior. The final epoch produced a batch of sample outputs that is very representative of the training dataset, which includes visual geographical features such as mountains, coast, and deltas.

Adam hyperparameter fine-tuning gave a great visualized reinforcement of lessons learning in class about optimizers. The two hyperparameters with the most effect on training our TerrainGAN are learning rates and mini-batch sizes. The importance of tuning learning rate and mini-batch size to expedite convergence and decrease training time can be visualized in the next two sections.
\begin{figure}
    \centering
    \includegraphics[scale=0.4]{2dlr.PNG}
    \caption{2-dimension visualization of learning rate effects}
    \label{2dlr}
\end{figure}
When taught the importance of learning rate(lr) in class, a 2 dimension graph comparison of low/correct/high learning rate is often shown. We define best lr = 0.0002 from previous runs, and correspondingly ran three training runs of TerrainGAN with 0.1*lr, lr, and 10*lr. We found close correlation between the 2 dimension graphs and our training process under these learning rates.
\begin{figure}
    \centering
    \includegraphics[scale=0.4]{3catsample2.PNG}
    \caption{Colorful output from learning rate too small (left), single feature rich heightmap from best learning rate (middle), red shaded output from learning rate too large (right)}
    \label{3catsamples2}
\end{figure}
Throughout the training process, Adam with 0.1*lr is struggling to even converge on producing grayscale tiles, visualizing the effect of using a small sub-optimal learning rate.

Adam with 10*lr will occasionally produce red shaded tile outputs, from otherwise a mostly converged generator. This phenomenon precisely visualizes the effect of big learning rate jumping out of the local minimum valley. Final epoch result also shows a less uniform distribution of pixels when compared to the best learning rate.

As one of the primary factors for optimizer convergence rate, it is best advised to try out a range of learning rates during the early stages of GAN training process and examine the results, before committing heavy computational resources.

Training time, overfitting, and GAN convergence are all closely tied to batch size tuning, as our training trials suggests. We used batch size 8, 16(optimal), and 64 to display our fine tuning findings.

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{3batches.PNG}
    \caption{Sample output from 3 different batch sizes}
    \label{3batches}
  \end{figure}

  From some randomly selected output tiles of each batch size, we can clearly see overfitting with batch size 8, which only uses 3 different values(black, gray, white) of the 256 value grayscale to try to fool the discriminator. On the other end, using batch size 64 against a dataset with only 4300 training data proved too feature diverse for the generator to learn over the 150 epoch period. The output heightmaps leave checkerboard artifacts from unfinished deconvolution [?].
  \begin{figure}
    \centering
    \includegraphics[scale=0.4]{runtime.PNG}
    \caption{Training time comparison between batch sizes}
    \label{runtime}
  \end{figure}
  
  When using DCGAN, a less stable version of LSGAN that utilizes binary cross entropy loss, large batch size of 64 will consequently completely fail to converge during the initial epochs of training, reiterating the importance of batch size tuning for GAN.
\begin{figure}
    \centering
    \includegraphics[scale=0.4]{dcganloss.PNG}
    \caption{Danger of complete divergence during initial epochs when using large batch sizes}
    \label{dcganloss}
\end{figure}  
\subsection*{Optimizers comparison}
% Here are the results of 150 epchos of SGD, SGDM and ADAM and their loss graph over iterations. Visually, SGD fail generate any geographical features we want such as mountains or lakes, valley, coasts. SGDM produced a largly reddish images which clearly shows that the output distribution is different from the training data. The generator loss of SGD and SGDM remains around 0.5, and the discrimintor stays around 0 which represents that the learning has not reached a point where generator is able to fool the discriminator. For adam graphs, this is another example of too big of learning rate which fail to converge.
% Throughout this project, we have been fine tuning optimizers SGD, SGDM, RMSProp. But hasn't found the right hyper parameter for these three optimizers to perform as good as the adam. It's possible it is caused by we didn't search enough hyper space to find the right choice, or it's possible these optimizer doesn't work well with GAN, althgou theoretically unlikely, but with limited training time/epchos, or it might be that the architecture needs to be adjusted for these optimizers.
Given the same learning rate and batch size, we compared the performence of four optimizers-SGD, SGD with momentum, RMSProp, and Adam traing for the same number of epchos. Figure\ref{sgdvssgdm} and \ref{adamvsrmsprop} shows the results of 150 epchos of SGD, SGDM and ADAM and their loss graph over iterations. Visually, SGD fails generating any geographical features that we want such as mountains or lakes, valley, coasts. SGDM produced a largly reddish images which clearly shows that the output distribution is different from the training data. The generator loss of SGD and SGDM remains around 0.5, and the discrimintor stays around 0 which represents that the learning has not reached a point where generator is able to fool the discriminator. For adam graphs, this is another example of too big of learning rate which fail to converge.
\begin{figure}
    \centering
    \includegraphics[scale=0.35]{sgdvssgdm.PNG}
    \caption{SGD vs. SGD with momentum}
    \label{sgdvssgdm}
\end{figure}
%We don't have enough time in this project to fine tune each optimizer to generate high quality [or meaningful] images. However, ADAM would be our choice for training GAN, as we can see from the fine tuning Adam section, it took us less time to search for a good learning rate and the method is just by simply decrease the magitude of the learning rate.
  
\section{Discussion about metrics}
As we have seen in the loss function graphs, the oscillation makes it harder to measure the success of the training progress. Currently, it is still an open problem to find good metrics for GAN training. Simply put, If you have a validation metric, you can probably use it as the objective function and optimize against it. Inception score \cite{salimans2016improved} and Frechet Inception Distance score (FID\cite{heusel2017gans}) are two state of the art metrics people use to measure GAN performances. However, both metrics don’t apply to our training data. Inception score requires categorically labeled data and FID requires a pretrained classification neural network. Inspired by the goal of GAN to learn the probability distribution from the training data and implicitly represent it using neural networks, we propose a new metric to measure the TerrainGAN performance: the KL-divergence between normalized intensity histogram of training images and generated images.

Before we present how we calculated this metric, we need first introduce the idea of image intensity histogram, which is a widely used technique in image processing. The definition states the histograms plots how many times each intensity value in an image occurs. Here we have a example of intensity histogram of a generated heightmap in figure  \ref{intensityhist}. We then produce the distribution over a large set of images by calculating the average occurrences of each intensity value and divide it by the total number of pixels over all images, and defining it as the normalized histogram. The normalizatoin process also remaps the occurences to the same range $[0,1]$ as the probability distribution. Figure \ref{traininten} shows the normalized intensity histogram of the training images, and figure \ref{adaminten} shows the normalized intensity histogram of 50 generated image using the neural net trained by Adam with lr .0002 and batch-size 16 for 150 epochs. 
\begin{figure}
    \centering
    \includegraphics[scale=0.45]{intensityhist.PNG}
    \caption{Image intensity histogram of a generated height map.}
    \label{intensityhist}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.55]{trainingintensity.PNG}
    \caption{Normalized image intensity histogram of the training dataset and an example training image.}
    \label{traininten}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.55]{adamintensity.PNG}
    \caption{Normalized image intensity histogram of generated images from LSGAN trained by ADAM with $lr=0.0002$ and $batchsize=16$ and a sample image. KL divergence of normalized intensity histogram equals $1.62$}
    \label{adaminten}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.55]{sgdintensity.PNG}
    \caption{Normalized image intensity histogram of generated images from LSGAN trained by SGD with $lr=0.0001$ and $batchsize=16$ and a sample image. KL divergence of normalized intensity histogram equals $+\infty$}
    \label{sgdinten}
\end{figure}


Given the normalized image intensity histograms of the training data and generated samples, we can calculate the KL divergence using equation (\ref{kldiv}), where $P(x)$ is the training data distribution and $Q(x)$ is the generated sample disctribution. Using the training dataset normalized histogram in figure \ref{traininten} as a reference distribution, figure \ref{adaminten} shows a normalized histogram with KL-divegence of $1.62$ and image \ref{sgdinten} shows a normalized histogram with KL-divergence of $+\infty$. We get a positive infinity since the training data and the generated data in figure \ref{sgdinten} have no overlap in normalized histogram. 
\begin{equation}
\label{kldiv}
  D_{KL}(P||Q)=\sum_{x\in \chi}{P(x)log(P(x)/Q(x))}
\end{equation}
We used this metric in measuring the quality of different hyperparameters when use Adam to train TerrainGAN. Figure \ref{klbar} shows a bar plot of the KL divergence of different hyperprameter settings of Adam after 150 ephchs. Visually, we  can see that the higher the metric, the harder you can provide geographic meaning to the generated images.
\begin{figure}
    \centering
    \includegraphics[scale=0.45]{klbar.PNG}
    \caption{Normalized histogram KL divergence of generated samples vs. different Adam learning rate and batch size}
    \label{klbar}
\end{figure}

%Compare performence of each optimizer:
%we need a metric to tell if the generated image is good or bad.
%Based on our goal, we want noticeable feasures like mountains or lakes, valley, coasts.

\section{Conclusion and next steps}
% We definitely learned a lot about different optimizers and a lot about GAN, DCGAN, LSGAN. shou huo hen da
In this optimizer study, we measured the performance of three different gradient based optimization algorithms used in our terrainGAN. We also highlighted the importance of choosing the right learning rate and batch size for proper GAN training. As a final thought on a month long training of GANs, look both visually and statistically at the generated data for metrics of success. More technical optimization method such as variable learning rates and noise injection will be next to tryout when training our future GANs. While working on TerrainGAN, we scoured information on the latest implementations of GANs such as styleGAN and progressiveGAN. We will be applying the lessons learned from this optimizer study into training art style transfer GANs and inpainting GANs.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,main}

\end{document}

